filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - ./pos-example.log
    fields:
      source: tcpserver
    json.keys_under_root: true
    json.overwrite_keys: true

#============================= Filebeat modules ===============================
filebeat.config.modules:
  # Glob pattern for configuration loading
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
  # Period on which files under path should be checked for changes
  #reload.period: 10s

#==================== Elasticsearch template setting ==========================
setup.template.settings:
  index.number_of_shards: 1
  #index.codec: best_compression
  #_source.enabled: false
#============================== Kibana =====================================

# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.
# This requires a Kibana endpoint configuration.
setup.kibana:
  host: "127.0.0.1:5601"
  # Kibana Space ID
  # ID of the Kibana Space into which the dashboards should be loaded. By default,
  # the Default Space will be used.
  #space.id:
#================================ Outputs =====================================
#-------------------------- Elasticsearch output ------------------------------
setup.template.name: "customname"
setup.template.pattern: "customname-*"

output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["127.0.0.1:9200"]
  #index: "customname-%{[agent.version]}-%{+yyyy.MM.dd}"
  indices:
    - index: tcpserver-%{+yyyy.MM.dd}
      when:
        has_fields: ['bj_time']
  # Optional protocol and basic auth credentials.
  #protocol: "https"
  username: "elastic"
  #password: "1992gw@espwd"
  password: "gao123456"
  #pipeline: handelWorkerNum

#================================ Processors =====================================

# Configure processors to enhance or manipulate events generated by the beat.


processors:
  #- add_host_metadata: ~
  #- add_cloud_metadata: ~
  - decode_json_fields:
      fields: ["__CONTENT__"]
      process_array: false
      max_depth: 2
      target: ""
      overwrite_keys: true
  - timestamp:
      field: time
      #target_field: created_at2
      layouts:
        - '2006-01-02T15:04:05Z'
        - '2006-01-02T15:04:05.999Z'
      timezone: America/Los_Angeles
  - drop_fields:
      fields: ["__CONTENT__", "kubernetes", "@metadata" ,"agent" , "docker", "metadata" ,"ecs" ,"stream","fields.source","fields.time",'time',"input.type","host.name"]

  - script:
      lang: javascript
      id: my_filter
      params:
        threshold: 15
      source: >
        var params = {threshold: 42};
        function register(scriptParams) {
            params = scriptParams;
        }

        function process(event) {
            var msg = event.Get('msg');
            if(msg && msg.indexOf("free worker num :") >=0){
              var num = msg.replace("free worker num :","");
              event.Put("worker_num", parseInt(num) );
            }else{
              event.Put("worker_num", -1);
            }

            var clientAddr = event.Get('client_addr')
            if(clientAddr){
              var arr = clientAddr.split(":")
              event.Put("client_ip",arr[0]);
            }
            var trace = event.Get('trace')
            if(trace && trace[0] && trace[0].Request &&trace[0].Request && trace[0].Request[0] ){
               var request =  trace[0].Request[0]
               event.Put("client_version", request.common.version );
               event.Put("net_type", request.common.net_type);
               event.Put("factory_id", request.common.factory_id);
            }
            var time = event.Get('log.time')
            event.Put('bj_time',time)
            event.Delete('trace')
            event.Delete('log')
            event.Delete('fields')
            event.Delete('host')
            event.Delete('input')
        }

